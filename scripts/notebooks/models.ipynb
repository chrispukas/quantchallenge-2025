{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017337e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B         C         D         E         F         G  \\\n",
      "0  0.128342 -0.150355 -0.627005 -0.585210 -0.256904 -0.002869 -0.458575   \n",
      "1  0.111149 -0.252733  0.042513 -0.556067 -0.127410 -0.232383  0.027353   \n",
      "2 -0.197761 -0.554981 -0.211682 -0.744126 -0.177530 -0.493903 -0.082152   \n",
      "3  0.129840 -0.296321  0.509336 -0.567932  0.716634 -0.344869  0.735615   \n",
      "4  0.022574 -0.342591  0.172240 -0.686743 -0.155533 -0.167701 -0.237098   \n",
      "\n",
      "          H         I         J  ...  K_neg    L_diff  L_pos  L_neg    M_diff  \\\n",
      "0 -1.425916 -0.378180 -0.661343  ...      1  0.000001      0      1 -0.000002   \n",
      "1  0.092018 -0.552405 -0.248102  ...      1 -0.235708      0      1  0.428053   \n",
      "2  0.675504 -0.632501  0.194827  ...      1 -0.304842      0      1  0.251707   \n",
      "3  1.312159 -0.426990  0.517681  ...      1  0.209939      0      1  0.239458   \n",
      "4  0.832055 -0.681470 -0.259371  ...      1 -0.073024      0      1 -0.321842   \n",
      "\n",
      "   M_pos  M_neg    N_diff  N_pos  N_neg  \n",
      "0      0      1 -0.000003      0      1  \n",
      "1      0      1  0.550895      1      0  \n",
      "2      1      0  0.078467      1      0  \n",
      "3      1      0  0.229390      1      0  \n",
      "4      1      0 -0.319696      1      0  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "torch.Size([80000, 58])\n",
      "torch.Size([80000, 2])\n",
      "Out of range by: 4 items!\n",
      "Out of range by: 8 items!\n",
      "Out of range by: 12 items!\n",
      "Out of range by: 16 items!\n",
      "Out of range by: 20 items!\n",
      "Out of range by: 24 items!\n",
      "Out of range by: 28 items!\n",
      "Out of range by: 32 items!\n",
      "Out of range by: 36 items!\n",
      "Out of range by: 40 items!\n",
      "Out of range by: 44 items!\n",
      "Out of range by: 48 items!\n",
      "Out of range by: 52 items!\n",
      "Out of range by: 56 items!\n",
      "Out of range by: 60 items!\n",
      "Out of range by: 64 items!\n",
      "Out of range by: 68 items!\n",
      "Out of range by: 72 items!\n",
      "Out of range by: 76 items!\n",
      "Out of range by: 80 items!\n",
      "Out of range by: 84 items!\n",
      "Out of range by: 88 items!\n",
      "Out of range by: 92 items!\n",
      "Out of range by: 96 items!\n",
      "Out of range by: 100 items!\n",
      "Out of range by: 104 items!\n",
      "Out of range by: 108 items!\n",
      "Out of range by: 112 items!\n",
      "Out of range by: 116 items!\n",
      "Out of range by: 120 items!\n",
      "Out of range by: 124 items!\n",
      "Initialized dataset torch.Size([20000, 128, 58]) torch.Size([20000, 128, 2])\n",
      "[2025-09-21 20:21:58.890900] Current batch item: 0, took 217 ms, current loss: 1.656673789024353, mean loss: 1.656673789024353\n",
      "[2025-09-21 20:22:16.082659] Current batch item: 250, took 17191 ms, current loss: 0.30859649181365967, mean loss: 0.6301080131554508\n",
      "[2025-09-21 20:22:33.374464] Current batch item: 500, took 17291 ms, current loss: 0.20644524693489075, mean loss: 0.4301520340635391\n",
      "1: AVG OVERALL LOSS: 0.386303076171875, FINAL LOSS: 0.2086774706840515, SMALLEST LOSS 0.11496136337518692, LARGEST LOSS: 30.19314193725586\n",
      "[2025-09-21 20:22:43.897547] Current batch item: 0, took 67 ms, current loss: 0.28165459632873535, mean loss: 0.28165459632873535\n",
      "[2025-09-21 20:23:01.314007] Current batch item: 250, took 17416 ms, current loss: 0.16731449961662292, mean loss: 0.19888559850563567\n",
      "[2025-09-21 20:23:18.796975] Current batch item: 500, took 17482 ms, current loss: 0.14526748657226562, mean loss: 0.18832165234817003\n",
      "2: AVG OVERALL LOSS: 0.18340529960393906, FINAL LOSS: 0.17599335312843323, SMALLEST LOSS 0.10035966336727142, LARGEST LOSS: 0.37894654273986816\n",
      "[2025-09-21 20:23:33.735002] Current batch item: 0, took 67 ms, current loss: 0.13515761494636536, mean loss: 0.13515761494636536\n",
      "[2025-09-21 20:23:51.265075] Current batch item: 250, took 17530 ms, current loss: 0.19536086916923523, mean loss: 0.1562206596432929\n",
      "[2025-09-21 20:24:08.887797] Current batch item: 500, took 17622 ms, current loss: 0.17197637259960175, mean loss: 0.1515284330991214\n",
      "3: AVG OVERALL LOSS: 0.15012751703262328, FINAL LOSS: 0.16430026292800903, SMALLEST LOSS 0.09461198002099991, LARGEST LOSS: 0.2477368414402008\n",
      "[2025-09-21 20:24:23.737200] Current batch item: 0, took 70 ms, current loss: 0.1304943859577179, mean loss: 0.1304943859577179\n",
      "[2025-09-21 20:24:41.396277] Current batch item: 250, took 17658 ms, current loss: 0.12673737108707428, mean loss: 0.13455588774258398\n",
      "[2025-09-21 20:24:59.066287] Current batch item: 500, took 17669 ms, current loss: 0.11067087948322296, mean loss: 0.1291951316589129\n",
      "4: AVG OVERALL LOSS: 0.12721920166015624, FINAL LOSS: 0.10170122981071472, SMALLEST LOSS 0.08091378211975098, LARGEST LOSS: 0.22805893421173096\n",
      "[2025-09-21 20:25:09.751386] Current batch item: 0, took 67 ms, current loss: 0.13223016262054443, mean loss: 0.13223016262054443\n",
      "[2025-09-21 20:25:27.442380] Current batch item: 250, took 17690 ms, current loss: 0.10530262440443039, mean loss: 0.11128720629262734\n",
      "[2025-09-21 20:25:45.110596] Current batch item: 500, took 17668 ms, current loss: 0.11605340242385864, mean loss: 0.10884388322066404\n",
      "5: AVG OVERALL LOSS: 0.10805650297403335, FINAL LOSS: 0.09361842274665833, SMALLEST LOSS 0.0789726972579956, LARGEST LOSS: 0.15668770670890808\n",
      "[2025-09-21 20:25:59.890280] Current batch item: 0, took 67 ms, current loss: 0.08932480216026306, mean loss: 0.08932480216026306\n",
      "[2025-09-21 20:26:17.570422] Current batch item: 250, took 17680 ms, current loss: 0.09014250338077545, mean loss: 0.09505260115363208\n",
      "[2025-09-21 20:26:35.255459] Current batch item: 500, took 17684 ms, current loss: 0.09441479295492172, mean loss: 0.0916083787759383\n",
      "6: AVG OVERALL LOSS: 0.09010616179704665, FINAL LOSS: 0.0872221440076828, SMALLEST LOSS 0.06162909418344498, LARGEST LOSS: 0.13608664274215698\n",
      "[2025-09-21 20:26:46.797098] Current batch item: 0, took 67 ms, current loss: 0.08358916640281677, mean loss: 0.08358916640281677\n",
      "[2025-09-21 20:27:04.475942] Current batch item: 250, took 17678 ms, current loss: 0.07003513723611832, mean loss: 0.07884165831652296\n",
      "[2025-09-21 20:27:22.148202] Current batch item: 500, took 17672 ms, current loss: 0.07686129212379456, mean loss: 0.07472117205044466\n",
      "7: AVG OVERALL LOSS: 0.07339805861711501, FINAL LOSS: 0.061856575310230255, SMALLEST LOSS 0.05441345274448395, LARGEST LOSS: 0.1053154394030571\n",
      "[2025-09-21 20:27:35.179524] Current batch item: 0, took 70 ms, current loss: 0.062075965106487274, mean loss: 0.062075965106487274\n",
      "[2025-09-21 20:27:52.844857] Current batch item: 250, took 17665 ms, current loss: 0.05553685128688812, mean loss: 0.0600530395796337\n",
      "[2025-09-21 20:28:10.526110] Current batch item: 500, took 17681 ms, current loss: 0.04808231443166733, mean loss: 0.05733647981208718\n",
      "8: AVG OVERALL LOSS: 0.05575525070428848, FINAL LOSS: 0.04731890559196472, SMALLEST LOSS 0.03855070471763611, LARGEST LOSS: 0.08049570769071579\n",
      "[2025-09-21 20:28:25.540648] Current batch item: 0, took 71 ms, current loss: 0.046387773007154465, mean loss: 0.046387773007154465\n",
      "[2025-09-21 20:28:43.228349] Current batch item: 250, took 17687 ms, current loss: 0.04013683646917343, mean loss: 0.04177970356081587\n",
      "[2025-09-21 20:29:00.904933] Current batch item: 500, took 17676 ms, current loss: 0.03811386227607727, mean loss: 0.039112256401164565\n",
      "9: AVG OVERALL LOSS: 0.03775133954584599, FINAL LOSS: 0.030376724898815155, SMALLEST LOSS 0.027313202619552612, LARGEST LOSS: 0.05854416266083717\n",
      "[2025-09-21 20:29:11.589250] Current batch item: 0, took 68 ms, current loss: 0.03031909093260765, mean loss: 0.03031909093260765\n",
      "[2025-09-21 20:29:29.259344] Current batch item: 250, took 17670 ms, current loss: 0.022559724748134613, mean loss: 0.028539562694342487\n",
      "[2025-09-21 20:29:46.947618] Current batch item: 500, took 17688 ms, current loss: 0.022496026009321213, mean loss: 0.02622122221631918\n",
      "10: AVG OVERALL LOSS: 0.02549885264635086, FINAL LOSS: 0.021377794444561005, SMALLEST LOSS 0.017846884205937386, LARGEST LOSS: 0.06616014987230301\n",
      "[2025-09-21 20:30:01.996853] Current batch item: 0, took 68 ms, current loss: 0.021169308573007584, mean loss: 0.021169308573007584\n",
      "[2025-09-21 20:30:19.670071] Current batch item: 250, took 17673 ms, current loss: 0.016039563342928886, mean loss: 0.019436577275632864\n",
      "[2025-09-21 20:30:37.347243] Current batch item: 500, took 17677 ms, current loss: 0.013477496802806854, mean loss: 0.01799982954411509\n",
      "11: AVG OVERALL LOSS: 0.017329411987960338, FINAL LOSS: 0.013290351256728172, SMALLEST LOSS 0.012065747752785683, LARGEST LOSS: 0.030491184443235397\n",
      "[2025-09-21 20:30:53.136692] Current batch item: 0, took 67 ms, current loss: 0.013964517042040825, mean loss: 0.013964517042040825\n",
      "[2025-09-21 20:31:10.818394] Current batch item: 250, took 17681 ms, current loss: 0.01638714410364628, mean loss: 0.015333833844508545\n",
      "[2025-09-21 20:31:28.493627] Current batch item: 500, took 17675 ms, current loss: 0.011446118354797363, mean loss: 0.015005594617622103\n",
      "12: AVG OVERALL LOSS: 0.014580792903900146, FINAL LOSS: 0.012505750171840191, SMALLEST LOSS 0.009593890979886055, LARGEST LOSS: 0.03700193762779236\n",
      "[2025-09-21 20:31:44.263975] Current batch item: 0, took 68 ms, current loss: 0.012667463161051273, mean loss: 0.012667463161051273\n",
      "[2025-09-21 20:32:01.949317] Current batch item: 250, took 17685 ms, current loss: 0.01269843615591526, mean loss: 0.014835306899659187\n",
      "[2025-09-21 20:32:19.625436] Current batch item: 500, took 17675 ms, current loss: 0.011374246329069138, mean loss: 0.01300071279319401\n",
      "13: AVG OVERALL LOSS: 0.012454681146144867, FINAL LOSS: 0.009807523339986801, SMALLEST LOSS 0.00804043747484684, LARGEST LOSS: 0.06334573775529861\n",
      "[2025-09-21 20:32:36.003339] Current batch item: 0, took 70 ms, current loss: 0.008171476423740387, mean loss: 0.008171476423740387\n",
      "[2025-09-21 20:32:53.672481] Current batch item: 250, took 17669 ms, current loss: 0.008970124647021294, mean loss: 0.010452858270683967\n",
      "[2025-09-21 20:33:11.356549] Current batch item: 500, took 17683 ms, current loss: 0.00934890192002058, mean loss: 0.009722327217719928\n",
      "14: AVG OVERALL LOSS: 0.009568252297490835, FINAL LOSS: 0.008107195608317852, SMALLEST LOSS 0.006650693714618683, LARGEST LOSS: 0.05759423226118088\n",
      "[2025-09-21 20:33:28.590012] Current batch item: 0, took 67 ms, current loss: 0.00785826612263918, mean loss: 0.00785826612263918\n",
      "[2025-09-21 20:33:46.257459] Current batch item: 250, took 17667 ms, current loss: 0.00792500376701355, mean loss: 0.007357120382195094\n",
      "[2025-09-21 20:34:03.928538] Current batch item: 500, took 17670 ms, current loss: 0.006271569058299065, mean loss: 0.007156901045733999\n",
      "15: AVG OVERALL LOSS: 0.007143295543640852, FINAL LOSS: 0.006088560912758112, SMALLEST LOSS 0.005568605847656727, LARGEST LOSS: 0.011216569691896439\n",
      "[2025-09-21 20:34:16.010010] Current batch item: 0, took 67 ms, current loss: 0.005423547700047493, mean loss: 0.005423547700047493\n",
      "[2025-09-21 20:34:33.685001] Current batch item: 250, took 17674 ms, current loss: 0.005708834156394005, mean loss: 0.006502831866779175\n",
      "[2025-09-21 20:34:51.360284] Current batch item: 500, took 17675 ms, current loss: 0.0077397688291966915, mean loss: 0.006725531934896212\n",
      "16: AVG OVERALL LOSS: 0.0069321134254336355, FINAL LOSS: 0.006870038341730833, SMALLEST LOSS 0.005122704431414604, LARGEST LOSS: 0.013421626761555672\n",
      "[2025-09-21 20:35:07.409472] Current batch item: 0, took 67 ms, current loss: 0.005588797852396965, mean loss: 0.005588797852396965\n",
      "[2025-09-21 20:35:25.101193] Current batch item: 250, took 17691 ms, current loss: 0.004926903173327446, mean loss: 0.0056089685907016\n",
      "[2025-09-21 20:35:42.785305] Current batch item: 500, took 17683 ms, current loss: 0.008596529252827168, mean loss: 0.005763822866272843\n",
      "17: AVG OVERALL LOSS: 0.005920648165047168, FINAL LOSS: 0.006784704513847828, SMALLEST LOSS 0.004117497242987156, LARGEST LOSS: 0.015048614703118801\n",
      "[2025-09-21 20:35:59.953389] Current batch item: 0, took 67 ms, current loss: 0.005698579363524914, mean loss: 0.005698579363524914\n",
      "[2025-09-21 20:36:17.627032] Current batch item: 250, took 17673 ms, current loss: 0.007255244068801403, mean loss: 0.013645618693880826\n",
      "[2025-09-21 20:36:35.322646] Current batch item: 500, took 17695 ms, current loss: 0.004377890843898058, mean loss: 0.009501541479529734\n",
      "18: AVG OVERALL LOSS: 0.0085287186101079, FINAL LOSS: 0.00467992527410388, SMALLEST LOSS 0.003606934566050768, LARGEST LOSS: 0.07112285494804382\n",
      "[2025-09-21 20:36:47.620112] Current batch item: 0, took 71 ms, current loss: 0.0041862246580421925, mean loss: 0.0041862246580421925\n",
      "[2025-09-21 20:37:05.293534] Current batch item: 250, took 17673 ms, current loss: 0.004013637080788612, mean loss: 0.004431222736152757\n",
      "[2025-09-21 20:37:22.973353] Current batch item: 500, took 17679 ms, current loss: 0.00497776223346591, mean loss: 0.004786001583895908\n",
      "19: AVG OVERALL LOSS: 0.004979011469706893, FINAL LOSS: 0.0046684034168720245, SMALLEST LOSS 0.003363816998898983, LARGEST LOSS: 0.016101796180009842\n",
      "[2025-09-21 20:37:37.762841] Current batch item: 0, took 67 ms, current loss: 0.007373373955488205, mean loss: 0.007373373955488205\n",
      "[2025-09-21 20:37:55.445289] Current batch item: 250, took 17682 ms, current loss: 0.0035006762482225895, mean loss: 0.004369391193353679\n",
      "[2025-09-21 20:38:13.123636] Current batch item: 500, took 17678 ms, current loss: 0.004034147132188082, mean loss: 0.0042097795068726926\n",
      "20: AVG OVERALL LOSS: 0.0042236450243741275, FINAL LOSS: 0.003324841847643256, SMALLEST LOSS 0.00289866141974926, LARGEST LOSS: 0.007610701024532318\n",
      "[2025-09-21 20:38:25.119566] Current batch item: 0, took 68 ms, current loss: 0.0036435751244425774, mean loss: 0.0036435751244425774\n",
      "[2025-09-21 20:38:42.801205] Current batch item: 250, took 17681 ms, current loss: 0.004506049677729607, mean loss: 0.004480591227497119\n",
      "[2025-09-21 20:39:00.486364] Current batch item: 500, took 17684 ms, current loss: 0.00397130474448204, mean loss: 0.004224347562702027\n",
      "21: AVG OVERALL LOSS: 0.004157575614377856, FINAL LOSS: 0.0033578909933567047, SMALLEST LOSS 0.0028512394055724144, LARGEST LOSS: 0.010728701949119568\n",
      "[2025-09-21 20:39:17.165622] Current batch item: 0, took 67 ms, current loss: 0.004272494465112686, mean loss: 0.004272494465112686\n",
      "[2025-09-21 20:39:34.838930] Current batch item: 250, took 17673 ms, current loss: 0.003094701562076807, mean loss: 0.0035143798083601247\n",
      "[2025-09-21 20:39:52.521582] Current batch item: 500, took 17682 ms, current loss: 0.0047433567233383656, mean loss: 0.0036955345017712807\n",
      "22: AVG OVERALL LOSS: 0.0037665785770863293, FINAL LOSS: 0.0036563104949891567, SMALLEST LOSS 0.0025396589189767838, LARGEST LOSS: 0.0067587001249194145\n",
      "[2025-09-21 20:40:08.857450] Current batch item: 0, took 67 ms, current loss: 0.00313645601272583, mean loss: 0.00313645601272583\n",
      "[2025-09-21 20:40:26.533239] Current batch item: 250, took 17675 ms, current loss: 0.002895866986364126, mean loss: 0.003826518599648934\n",
      "[2025-09-21 20:40:44.205120] Current batch item: 500, took 17671 ms, current loss: 0.0057183802127838135, mean loss: 0.007125710475513202\n",
      "23: AVG OVERALL LOSS: 0.0074703177943825725, FINAL LOSS: 0.006588914897292852, SMALLEST LOSS 0.00232408894225955, LARGEST LOSS: 0.1016560047864914\n",
      "[2025-09-21 20:41:01.428153] Current batch item: 0, took 67 ms, current loss: 0.004440042190253735, mean loss: 0.004440042190253735\n",
      "[2025-09-21 20:41:19.096654] Current batch item: 250, took 17668 ms, current loss: 0.003359124530106783, mean loss: 0.0033712749150526\n",
      "[2025-09-21 20:41:36.772945] Current batch item: 500, took 17676 ms, current loss: 0.003204676555469632, mean loss: 0.004722440538150881\n",
      "24: AVG OVERALL LOSS: 0.004372018764168024, FINAL LOSS: 0.0030640591867268085, SMALLEST LOSS 0.0021292250603437424, LARGEST LOSS: 0.01823357678949833\n",
      "[2025-09-21 20:41:51.665458] Current batch item: 0, took 68 ms, current loss: 0.0027527902275323868, mean loss: 0.0027527902275323868\n",
      "[2025-09-21 20:42:09.356900] Current batch item: 250, took 17691 ms, current loss: 0.002252447186037898, mean loss: 0.0029701031855793587\n",
      "[2025-09-21 20:42:27.041236] Current batch item: 500, took 17684 ms, current loss: 0.002381985541433096, mean loss: 0.002785409149619009\n",
      "25: AVG OVERALL LOSS: 0.002796346406266093, FINAL LOSS: 0.0027562740724533796, SMALLEST LOSS 0.0019925259985029697, LARGEST LOSS: 0.008999854326248169\n",
      "[2025-09-21 20:42:38.288783] Current batch item: 0, took 71 ms, current loss: 0.0023698771838098764, mean loss: 0.0023698771838098764\n",
      "[2025-09-21 20:42:55.960459] Current batch item: 250, took 17671 ms, current loss: 0.002685240004211664, mean loss: 0.002511612833959915\n",
      "[2025-09-21 20:43:13.647016] Current batch item: 500, took 17686 ms, current loss: 0.003144418355077505, mean loss: 0.002545898167013066\n",
      "26: AVG OVERALL LOSS: 0.0025754718447104094, FINAL LOSS: 0.0024105445481836796, SMALLEST LOSS 0.0018869107589125633, LARGEST LOSS: 0.005031256005167961\n",
      "[2025-09-21 20:43:28.613390] Current batch item: 0, took 67 ms, current loss: 0.0022274539805948734, mean loss: 0.0022274539805948734\n",
      "[2025-09-21 20:43:46.285349] Current batch item: 250, took 17671 ms, current loss: 0.00277470494620502, mean loss: 0.0027021169226497887\n",
      "[2025-09-21 20:44:03.964206] Current batch item: 500, took 17678 ms, current loss: 0.0022174501791596413, mean loss: 0.002945773738962477\n",
      "27: AVG OVERALL LOSS: 0.0028330850576981904, FINAL LOSS: 0.002424130216240883, SMALLEST LOSS 0.0018038866110146046, LARGEST LOSS: 0.006237388588488102\n",
      "[2025-09-21 20:44:16.167624] Current batch item: 0, took 70 ms, current loss: 0.0028370441868901253, mean loss: 0.0028370441868901253\n",
      "[2025-09-21 20:44:33.856345] Current batch item: 250, took 17688 ms, current loss: 0.00259110564365983, mean loss: 0.0029679570918479765\n",
      "[2025-09-21 20:44:51.535176] Current batch item: 500, took 17678 ms, current loss: 0.002188070910051465, mean loss: 0.002732437410245012\n",
      "28: AVG OVERALL LOSS: 0.002871132918074727, FINAL LOSS: 0.004770174622535706, SMALLEST LOSS 0.0018669026903808117, LARGEST LOSS: 0.008384937420487404\n",
      "[2025-09-21 20:45:06.602812] Current batch item: 0, took 67 ms, current loss: 0.0026626363396644592, mean loss: 0.0026626363396644592\n",
      "[2025-09-21 20:45:24.280510] Current batch item: 250, took 17677 ms, current loss: 0.0020766067318618298, mean loss: 0.002567703638084797\n",
      "[2025-09-21 20:45:41.969255] Current batch item: 500, took 17688 ms, current loss: 0.0025616176426410675, mean loss: 0.0023681549232233753\n",
      "29: AVG OVERALL LOSS: 0.002378800743073225, FINAL LOSS: 0.002494922373443842, SMALLEST LOSS 0.0016472829738631845, LARGEST LOSS: 0.003976807929575443\n",
      "[2025-09-21 20:45:58.451571] Current batch item: 0, took 67 ms, current loss: 0.002160301897674799, mean loss: 0.002160301897674799\n",
      "[2025-09-21 20:46:16.128687] Current batch item: 250, took 17677 ms, current loss: 0.0030062212608754635, mean loss: 0.002477402498285134\n",
      "[2025-09-21 20:46:33.815759] Current batch item: 500, took 17686 ms, current loss: 0.0035995561629533768, mean loss: 0.00279861193186531\n",
      "30: AVG OVERALL LOSS: 0.002809227637760341, FINAL LOSS: 0.0020186605397611856, SMALLEST LOSS 0.0017574389930814505, LARGEST LOSS: 0.007113534025847912\n",
      "[2025-09-21 20:46:43.560775] Current batch item: 0, took 67 ms, current loss: 0.001797630451619625, mean loss: 0.001797630451619625\n",
      "[2025-09-21 20:47:01.239895] Current batch item: 250, took 17679 ms, current loss: 0.0023385253734886646, mean loss: 0.0020759398061077137\n",
      "[2025-09-21 20:47:18.923646] Current batch item: 500, took 17683 ms, current loss: 0.002464976394549012, mean loss: 0.0022301702755977383\n",
      "31: AVG OVERALL LOSS: 0.002269899711199105, FINAL LOSS: 0.002380805555731058, SMALLEST LOSS 0.0015792632475495338, LARGEST LOSS: 0.003966406919062138\n",
      "[2025-09-21 20:47:30.094421] Current batch item: 0, took 67 ms, current loss: 0.001934556057676673, mean loss: 0.001934556057676673\n",
      "[2025-09-21 20:47:47.792698] Current batch item: 250, took 17698 ms, current loss: 0.0015373843489214778, mean loss: 0.002219973739136411\n",
      "[2025-09-21 20:48:05.490513] Current batch item: 500, took 17697 ms, current loss: 0.001966970507055521, mean loss: 0.0023877296818442598\n",
      "32: AVG OVERALL LOSS: 0.002331002214923501, FINAL LOSS: 0.0020344010554254055, SMALLEST LOSS 0.0015220851637423038, LARGEST LOSS: 0.0067336284555494785\n",
      "[2025-09-21 20:48:15.252720] Current batch item: 0, took 70 ms, current loss: 0.0021762950345873833, mean loss: 0.0021762950345873833\n",
      "[2025-09-21 20:48:32.946626] Current batch item: 250, took 17693 ms, current loss: 0.0016147427959367633, mean loss: 0.0019362376519549355\n",
      "[2025-09-21 20:48:50.640665] Current batch item: 500, took 17693 ms, current loss: 0.0022198185324668884, mean loss: 0.0019860810248419165\n",
      "33: AVG OVERALL LOSS: 0.0020699560517445205, FINAL LOSS: 0.0025707639288157225, SMALLEST LOSS 0.001362970215268433, LARGEST LOSS: 0.004850446246564388\n",
      "[2025-09-21 20:49:00.971123] Current batch item: 0, took 67 ms, current loss: 0.003030919237062335, mean loss: 0.003030919237062335\n",
      "[2025-09-21 20:49:18.651181] Current batch item: 250, took 17679 ms, current loss: 0.002532671205699444, mean loss: 0.002372111358486888\n",
      "[2025-09-21 20:49:36.337753] Current batch item: 500, took 17686 ms, current loss: 0.0027460656128823757, mean loss: 0.002107894132908307\n",
      "34: AVG OVERALL LOSS: 0.002094667561724782, FINAL LOSS: 0.001620398135855794, SMALLEST LOSS 0.0013074297457933426, LARGEST LOSS: 0.003864588215947151\n",
      "[2025-09-21 20:49:46.088362] Current batch item: 0, took 67 ms, current loss: 0.0017811218276619911, mean loss: 0.0017811218276619911\n",
      "[2025-09-21 20:50:03.790497] Current batch item: 250, took 17702 ms, current loss: 0.0018679467029869556, mean loss: 0.001960846659471374\n",
      "[2025-09-21 20:50:21.472201] Current batch item: 500, took 17681 ms, current loss: 0.002431308850646019, mean loss: 0.003930299799143896\n",
      "35: AVG OVERALL LOSS: 0.0036885481374338267, FINAL LOSS: 0.0026645702309906483, SMALLEST LOSS 0.0013690481428056955, LARGEST LOSS: 0.029163967818021774\n",
      "Evaluation mode activated, created training items\n",
      "Out of range by: 4 items!\n",
      "torch.Size([16000, 2]) 16000 torch.Size([16000, 1]) 2\n",
      "tensor([[ 0.7110],\n",
      "        [-0.5542],\n",
      "        [-0.3337],\n",
      "        ...,\n",
      "        [ 0.3290],\n",
      "        [ 0.1739],\n",
      "        [ 0.1755]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from qch2025.pkg.models.LSTM import network as network\n",
    "from qch2025.pkg.models.LSTM import func as func\n",
    "\n",
    "from qch2025.pkg.dataset import DS\n",
    "\n",
    "import qch2025.pkg.plotting as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "n_epochs = 35\n",
    "window_size = 128\n",
    "batch_size = 32\n",
    "window_steps = 4 # Minimize the overlap\n",
    "attn_heads = 4\n",
    "\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.1\n",
    "\n",
    "r2_priority = 0.1\n",
    "\n",
    "dtype = torch.float32\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Training\n",
    "train_dataset = DS(dataset_path=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/train.csv\",\n",
    "                   window_size=window_size,\n",
    "                   window_steps=window_steps,\n",
    "                   dtype=dtype)\n",
    "evaluate_dataset = DS(dataset_path=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/train.csv\",\n",
    "                   window_size=window_size,\n",
    "                   window_steps=window_steps,\n",
    "                   dtype=dtype)\n",
    "\n",
    "rnn = network.RNN(input_size=58, \n",
    "                  hidden_size=1024, \n",
    "                  lstm_layers=3, \n",
    "                  lstm_dropout=dropout, \n",
    "                  attn_heads=attn_heads, \n",
    "                  output_size=2, \n",
    "                  device=device, \n",
    "                  dtype=dtype)\n",
    "\n",
    "losses = func.train(model=rnn, \n",
    "                    dataset=train_dataset, \n",
    "                    epochs=n_epochs, \n",
    "                    learning_rate=learning_rate, \n",
    "                    decay=r2_priority)\n",
    "\n",
    "#plt.plot_line(np.arange(1, len(losses)+1), losses)\n",
    "\n",
    "\n",
    "#checkpoint = torch.load(\"/home/ubuntu/repos/quantchallenge-2025/weights/weights.pth\", weights_only=True)\n",
    "#rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "eval = DS(dataset_path=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test.csv\",\n",
    "          window_size=window_size,\n",
    "          window_steps=window_size,\n",
    "          eval=True,\n",
    "          device=torch.device(\"cuda\"),\n",
    "          dtype=dtype) # Create non-overlapping windows\n",
    "y1, y2 = func.eval(model=rnn, \n",
    "                   dataset=eval, \n",
    "                   batch_size=batch_size)\n",
    "\n",
    "df = pd.DataFrame({\"Y1\": y1, \"Y2\": y2})\n",
    "df.index.name=\"id\"\n",
    "df.index= df.index + 1\n",
    "\n",
    "actual = len(eval.df[\"A\"])\n",
    "df1_trunc = df.head(actual)\n",
    "\n",
    "df1_trunc.to_csv(f\"/home/ubuntu/repos/quantchallenge-2025/qch2025/outputs/{datetime.datetime.now()}_predicted.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154d515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c7b387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation mode activated, created training items\n",
      "Out of range by: 4 items!\n",
      "torch.Size([16000, 2]) 16000 torch.Size([16000, 1]) 2\n",
      "tensor([[ 0.6544],\n",
      "        [-0.5561],\n",
      "        [-0.2648],\n",
      "        ...,\n",
      "        [ 0.4971],\n",
      "        [ 0.2935],\n",
      "        [ 0.1885]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from qch2025.pkg.models.RNN import network as network\n",
    "from qch2025.pkg.models.RNN import func as func\n",
    "\n",
    "from qch2025.pkg.dataset import DS\n",
    "\n",
    "import qch2025.pkg.plotting as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "n_epochs = 35\n",
    "window_size = 128\n",
    "batch_size = 32\n",
    "window_steps = 4 # Minimize the overlap\n",
    "attn_heads = 4\n",
    "\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.2\n",
    "\n",
    "r2_priority = 0.1\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "rnn = network.RNN(58, 512, 3, dropout, attn_heads, 2, device=torch.device(\"cuda\"), dtype=dtype)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"/home/ubuntu/repos/quantchallenge-2025/weights/weights_0.0023788.pth\", weights_only=True)\n",
    "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "eval = DS(dataset_path=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test.csv\",\n",
    "          dataset_path_2=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test_new.csv\",\n",
    "                    window_size=window_size,\n",
    "                    window_steps=window_size,\n",
    "                    eval=True,\n",
    "                    device=torch.device(\"cuda\"),\n",
    "                    dtype=dtype)\n",
    "y1, y2 = func.eval(rnn, eval, batch_size=32)\n",
    "df = pd.DataFrame({\"Y1\": y1, \"Y2\": y2})\n",
    "df.index.name=\"id\"\n",
    "df.index= df.index + 1\n",
    "\n",
    "actual = len(eval.df[\"A\"])\n",
    "df1_trunc = df.head(actual)\n",
    "\n",
    "df1_trunc.to_csv(f\"/home/ubuntu/repos/quantchallenge-2025/qch2025/outputs/{datetime.datetime.now()}_predicted.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
