{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017337e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000, 16])\n",
      "torch.Size([80000, 2])\n",
      "Out of range by: 4 items!\n",
      "Out of range by: 8 items!\n",
      "Out of range by: 12 items!\n",
      "Out of range by: 16 items!\n",
      "Out of range by: 20 items!\n",
      "Out of range by: 24 items!\n",
      "Out of range by: 28 items!\n",
      "Out of range by: 32 items!\n",
      "Out of range by: 36 items!\n",
      "Out of range by: 40 items!\n",
      "Out of range by: 44 items!\n",
      "Out of range by: 48 items!\n",
      "Out of range by: 52 items!\n",
      "Out of range by: 56 items!\n",
      "Out of range by: 60 items!\n",
      "Out of range by: 64 items!\n",
      "Out of range by: 68 items!\n",
      "Out of range by: 72 items!\n",
      "Out of range by: 76 items!\n",
      "Out of range by: 80 items!\n",
      "Out of range by: 84 items!\n",
      "Out of range by: 88 items!\n",
      "Out of range by: 92 items!\n",
      "Out of range by: 96 items!\n",
      "Out of range by: 100 items!\n",
      "Out of range by: 104 items!\n",
      "Out of range by: 108 items!\n",
      "Out of range by: 112 items!\n",
      "Out of range by: 116 items!\n",
      "Out of range by: 120 items!\n",
      "Out of range by: 124 items!\n",
      "Initialized dataset torch.Size([20000, 128, 16]) torch.Size([20000, 128, 2])\n",
      "[2025-09-21 19:45:45.409832] Current batch item: 0, took 68 ms, current loss: 2.2620906829833984, mean loss: 2.2620906829833984\n",
      "[2025-09-21 19:46:02.692172] Current batch item: 250, took 17282 ms, current loss: 0.2535102069377899, mean loss: 0.6369548680416617\n",
      "[2025-09-21 19:46:20.070382] Current batch item: 500, took 17378 ms, current loss: 0.21596679091453552, mean loss: 0.4397931572443949\n",
      "1: AVG OVERALL LOSS: 0.3970399487733841, FINAL LOSS: 0.32532012462615967, SMALLEST LOSS 0.12244032323360443, LARGEST LOSS: 26.508806228637695\n",
      "[2025-09-21 19:46:30.528408] Current batch item: 0, took 67 ms, current loss: 0.18832600116729736, mean loss: 0.18832600116729736\n",
      "[2025-09-21 19:46:48.012747] Current batch item: 250, took 17484 ms, current loss: 0.2359500676393509, mean loss: 0.2181770817573327\n",
      "[2025-09-21 19:47:05.641470] Current batch item: 500, took 17628 ms, current loss: 0.20523793995380402, mean loss: 0.20625107879648188\n",
      "2: AVG OVERALL LOSS: 0.2023178488135338, FINAL LOSS: 0.17724968492984772, SMALLEST LOSS 0.1083683967590332, LARGEST LOSS: 0.4212028980255127\n",
      "[2025-09-21 19:47:16.330646] Current batch item: 0, took 67 ms, current loss: 0.17682117223739624, mean loss: 0.17682117223739624\n",
      "[2025-09-21 19:47:33.988673] Current batch item: 250, took 17657 ms, current loss: 0.16626012325286865, mean loss: 0.1817909852560773\n",
      "[2025-09-21 19:47:51.656195] Current batch item: 500, took 17667 ms, current loss: 0.14528557658195496, mean loss: 0.17475726790056972\n",
      "3: AVG OVERALL LOSS: 0.1713427553296089, FINAL LOSS: 0.16063471138477325, SMALLEST LOSS 0.10421256721019745, LARGEST LOSS: 0.31923753023147583\n",
      "[2025-09-21 19:48:08.239926] Current batch item: 0, took 67 ms, current loss: 0.14973141252994537, mean loss: 0.14973141252994537\n",
      "[2025-09-21 19:48:25.919623] Current batch item: 250, took 17679 ms, current loss: 0.175693541765213, mean loss: 0.15416481212315805\n",
      "[2025-09-21 19:48:43.590794] Current batch item: 500, took 17671 ms, current loss: 0.16935977339744568, mean loss: 0.15051716751978544\n",
      "4: AVG OVERALL LOSS: 0.1495165179014206, FINAL LOSS: 0.1592051386833191, SMALLEST LOSS 0.09596913307905197, LARGEST LOSS: 0.2208409607410431\n",
      "[2025-09-21 19:48:54.214271] Current batch item: 0, took 67 ms, current loss: 0.12386112660169601, mean loss: 0.12386112660169601\n",
      "[2025-09-21 19:49:11.872805] Current batch item: 250, took 17658 ms, current loss: 0.14870169758796692, mean loss: 0.13781931194888644\n",
      "[2025-09-21 19:49:29.557242] Current batch item: 500, took 17684 ms, current loss: 0.16328300535678864, mean loss: 0.1331991268370204\n",
      "5: AVG OVERALL LOSS: 0.1311871589422226, FINAL LOSS: 0.10605311393737793, SMALLEST LOSS 0.08568193763494492, LARGEST LOSS: 0.21238923072814941\n",
      "[2025-09-21 19:49:40.239634] Current batch item: 0, took 68 ms, current loss: 0.10355700552463531, mean loss: 0.10355700552463531\n",
      "[2025-09-21 19:49:57.913447] Current batch item: 250, took 17673 ms, current loss: 0.10133300721645355, mean loss: 0.11868222120154902\n",
      "[2025-09-21 19:50:15.589317] Current batch item: 500, took 17675 ms, current loss: 0.10002896189689636, mean loss: 0.11552947991622423\n",
      "6: AVG OVERALL LOSS: 0.11417452926635742, FINAL LOSS: 0.11825932562351227, SMALLEST LOSS 0.08107399195432663, LARGEST LOSS: 0.16959026455879211\n",
      "[2025-09-21 19:50:26.287429] Current batch item: 0, took 68 ms, current loss: 0.10664493590593338, mean loss: 0.10664493590593338\n",
      "[2025-09-21 19:50:43.981439] Current batch item: 250, took 17693 ms, current loss: 0.09613741934299469, mean loss: 0.10199598323895162\n",
      "[2025-09-21 19:51:01.657163] Current batch item: 500, took 17675 ms, current loss: 0.08084806054830551, mean loss: 0.09849381967338021\n",
      "7: AVG OVERALL LOSS: 0.09680412871837615, FINAL LOSS: 0.08340457081794739, SMALLEST LOSS 0.06607352197170258, LARGEST LOSS: 0.14109039306640625\n",
      "[2025-09-21 19:51:12.342202] Current batch item: 0, took 67 ms, current loss: 0.08659467101097107, mean loss: 0.08659467101097107\n",
      "[2025-09-21 19:51:30.013289] Current batch item: 250, took 17671 ms, current loss: 0.07304838299751282, mean loss: 0.08625691550305165\n",
      "[2025-09-21 19:51:47.705484] Current batch item: 500, took 17692 ms, current loss: 0.08414990454912186, mean loss: 0.08260546180748654\n",
      "8: AVG OVERALL LOSS: 0.08060880280137062, FINAL LOSS: 0.0713459700345993, SMALLEST LOSS 0.05630757659673691, LARGEST LOSS: 0.12754938006401062\n",
      "[2025-09-21 19:51:58.404631] Current batch item: 0, took 67 ms, current loss: 0.08013349771499634, mean loss: 0.08013349771499634\n",
      "[2025-09-21 19:52:16.074381] Current batch item: 250, took 17669 ms, current loss: 0.05787493661046028, mean loss: 0.06674234236616537\n",
      "[2025-09-21 19:52:33.753779] Current batch item: 500, took 17679 ms, current loss: 0.05183225870132446, mean loss: 0.06344965483970985\n",
      "9: AVG OVERALL LOSS: 0.06150685847997665, FINAL LOSS: 0.05343133956193924, SMALLEST LOSS 0.04419271647930145, LARGEST LOSS: 0.08515023440122604\n",
      "[2025-09-21 19:52:44.452458] Current batch item: 0, took 68 ms, current loss: 0.05027715116739273, mean loss: 0.05027715116739273\n",
      "[2025-09-21 19:53:02.136824] Current batch item: 250, took 17684 ms, current loss: 0.04321669787168503, mean loss: 0.049523965162466245\n",
      "[2025-09-21 19:53:19.813139] Current batch item: 500, took 17676 ms, current loss: 0.03996061533689499, mean loss: 0.04604724343278689\n",
      "10: AVG OVERALL LOSS: 0.04484564265608788, FINAL LOSS: 0.04295371472835541, SMALLEST LOSS 0.03280028700828552, LARGEST LOSS: 0.06681779772043228\n",
      "[2025-09-21 19:53:30.412375] Current batch item: 0, took 68 ms, current loss: 0.03788479417562485, mean loss: 0.03788479417562485\n",
      "[2025-09-21 19:53:48.082845] Current batch item: 250, took 17670 ms, current loss: 0.034022603183984756, mean loss: 0.03575483380828008\n",
      "[2025-09-21 19:54:05.777147] Current batch item: 500, took 17694 ms, current loss: 0.027053970843553543, mean loss: 0.03244438893840223\n",
      "11: AVG OVERALL LOSS: 0.03128426414430142, FINAL LOSS: 0.02440790832042694, SMALLEST LOSS 0.021914593875408173, LARGEST LOSS: 0.06050769239664078\n",
      "[2025-09-21 19:54:16.449643] Current batch item: 0, took 68 ms, current loss: 0.023913636803627014, mean loss: 0.023913636803627014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     27\u001b[39m train_dataset = DS(dataset_path=\u001b[33m\"\u001b[39m\u001b[33m/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/train.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m                    dataset_path_2=\u001b[33m\"\u001b[39m\u001b[33m/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/train_new.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m                         window_size=window_size,\n\u001b[32m     30\u001b[39m                         window_steps=window_steps,\n\u001b[32m     31\u001b[39m                         dtype=dtype)\n\u001b[32m     33\u001b[39m rnn = network.RNN(\u001b[32m16\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m3\u001b[39m, dropout, attn_heads, \u001b[32m2\u001b[39m, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m), dtype=dtype)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m losses = \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr2_priority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m#plt.plot_line(np.arange(1, len(losses)+1), losses)\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m#checkpoint = torch.load(\"/home/ubuntu/repos/quantchallenge-2025/weights/weights.pth\", weights_only=True)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m#rnn.load_state_dict(checkpoint['model_state_dict'])\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28meval\u001b[39m = DS(dataset_path=\u001b[33m\"\u001b[39m\u001b[33m/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m           dataset_path_2=\u001b[33m\"\u001b[39m\u001b[33m/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test_new.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m                     window_size=window_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m                     device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     48\u001b[39m                     dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/quantchallenge-2025/qch2025/pkg/models/RNN/func.py:36\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataset, epochs, batch_size, decay, learning_rate)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m#loss = combined_loss(pred_out, target, alpha_decay=decay)\u001b[39;00m\n\u001b[32m     34\u001b[39m loss = cr(pred_out, target)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m3\u001b[39m)\n\u001b[32m     38\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qch/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qch/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qch/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from qch2025.pkg.models.RNN import network as network\n",
    "from qch2025.pkg.models.RNN import func as func\n",
    "\n",
    "from qch2025.pkg.dataset import DS\n",
    "\n",
    "import qch2025.pkg.plotting as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "n_epochs = 35\n",
    "window_size = 128\n",
    "batch_size = 32\n",
    "window_steps = 4 # Minimize the overlap\n",
    "attn_heads = 4\n",
    "\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.1\n",
    "\n",
    "r2_priority = 0.1\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "# Training\n",
    "train_dataset = DS(dataset_path=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/train.csv\",\n",
    "                   dataset_path_2=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/train_new.csv\",\n",
    "                        window_size=window_size,\n",
    "                        window_steps=window_steps,\n",
    "                        dtype=dtype)\n",
    "\n",
    "rnn = network.RNN(16, 1024, 3, dropout, attn_heads, 2, device=torch.device(\"cuda\"), dtype=dtype)\n",
    "losses = func.train(rnn, dataset=train_dataset, epochs=n_epochs, learning_rate=learning_rate, decay=r2_priority)\n",
    "\n",
    "#plt.plot_line(np.arange(1, len(losses)+1), losses)\n",
    "\n",
    "\n",
    "#checkpoint = torch.load(\"/home/ubuntu/repos/quantchallenge-2025/weights/weights.pth\", weights_only=True)\n",
    "#rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "eval = DS(dataset_path=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test.csv\",\n",
    "          dataset_path_2=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test_new.csv\",\n",
    "                    window_size=window_size,\n",
    "                    window_steps=window_size,\n",
    "                    eval=True,\n",
    "                    device=torch.device(\"cuda\"),\n",
    "                    dtype=dtype)\n",
    "y1, y2 = func.eval(rnn, eval, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"Y1\": y1, \"Y2\": y2})\n",
    "df.index.name=\"id\"\n",
    "df.index= df.index + 1\n",
    "\n",
    "actual = len(eval.df[\"A\"])\n",
    "df1_trunc = df.head(actual)\n",
    "\n",
    "df1_trunc.to_csv(f\"/home/ubuntu/repos/quantchallenge-2025/qch2025/outputs/{datetime.datetime.now()}_predicted.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154d515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c7b387",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RNN:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 14]) from checkpoint, the shape in current model is torch.Size([2048, 15]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([2048, 14]) from checkpoint, the shape in current model is torch.Size([2048, 15]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m rnn = network.RNN(\u001b[32m15\u001b[39m, \u001b[32m512\u001b[39m, \u001b[32m2\u001b[39m, dropout, attn_heads, \u001b[32m2\u001b[39m, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m), dtype=dtype)\n\u001b[32m     29\u001b[39m checkpoint = torch.load(\u001b[33m\"\u001b[39m\u001b[33m/home/ubuntu/repos/quantchallenge-2025/weights/weights.pth\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28meval\u001b[39m = DS(dataset_path=\u001b[33m\"\u001b[39m\u001b[33m/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m                     window_size=window_size,\n\u001b[32m     34\u001b[39m                     window_steps=\u001b[32m1\u001b[39m,\n\u001b[32m     35\u001b[39m                     \u001b[38;5;28meval\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     36\u001b[39m                     device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     37\u001b[39m                     dtype=dtype)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28meval\u001b[39m.ids.shape, \u001b[38;5;28meval\u001b[39m.train.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qch/lib/python3.13/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for RNN:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 14]) from checkpoint, the shape in current model is torch.Size([2048, 15]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([2048, 14]) from checkpoint, the shape in current model is torch.Size([2048, 15])."
     ]
    }
   ],
   "source": [
    "from qch2025.pkg.models.RNN import network as network\n",
    "from qch2025.pkg.models.RNN import func as func\n",
    "\n",
    "from qch2025.pkg.dataset import DS\n",
    "\n",
    "import qch2025.pkg.plotting as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "n_epochs = 30\n",
    "window_size = 128\n",
    "batch_size = 32\n",
    "window_steps = 4\n",
    "attn_heads = 2\n",
    "\n",
    "learning_rate = 0.0005\n",
    "dropout = 0.05\n",
    "\n",
    "r2_priority = 0.1\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "rnn = network.RNN(15, 512, 2, dropout, attn_heads, 2, device=torch.device(\"cuda\"), dtype=dtype)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"/home/ubuntu/repos/quantchallenge-2025/weights/weights.pth\", weights_only=True)\n",
    "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "eval = DS(dataset_path=\"/home/ubuntu/repos/quantchallenge-2025/qch2025/dataset/test.csv\",\n",
    "                    window_size=window_size,\n",
    "                    window_steps=1,\n",
    "                    eval=True,\n",
    "                    device=torch.device(\"cuda\"),\n",
    "                    dtype=dtype)\n",
    "print(eval.ids.shape, eval.train.shape)\n",
    "y1, y2 = func.eval(rnn, eval)\n",
    "\n",
    "ids = np.arrange(1, len(y1)+1)\n",
    "print(len(ids))\n",
    "print(len(y1), y1.shape)\n",
    "\n",
    "df = pd.DataFrame({\"Y1\": y1, \"Y2\": y2})\n",
    "df.index.name=\"id\"\n",
    "df.index= df.index + 1\n",
    "df.to_csv(f\"/home/ubuntu/repos/quantchallenge-2025/qch2025/outputs/{datetime.datetime.now()}_predicted.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
